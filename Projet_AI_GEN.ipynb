{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7e04a3-242e-4504-8550-20142c6d30e4",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "   <a>\n",
    "    <img src=\"https://readme-typing-svg.demolab.com?font=Georgia&size=42&duration=2000&pause=100&multiline=true&width=1000&height=200&lines=Yacine+BERKANI+%7C+Anass+EL+ADI;Master+2+Big+Data+Université+Paris+8+;Projet+IA+Générative\" alt=\"Typing SVG\" />\n",
    "</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa7589-402d-4672-971a-b3f6af00dc9b",
   "metadata": {},
   "source": [
    "# 1. Configuration de l'environnement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94012c7c-0778-43be-be55-5fba76fca449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.47.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: streamlit in /opt/anaconda3/lib/python3.12/site-packages (1.32.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (10.3.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (13.3.5)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961f917b-e3a1-4e25-8c65-d87593e1ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 téléchargé avec succès !\n",
      "GPT-Neo téléchargé avec succès !\n"
     ]
    }
   ],
   "source": [
    "# Importation des classes nécessaires depuis la bibliothèque `transformers`\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Définition de la fonction `telecharger_modeles` qui télécharge les modèles et leurs tokenizers\n",
    "def telecharger_modeles():\n",
    "    # Exemple pour GPT-2\n",
    "    \n",
    "    # Nom du modèle GPT-2 à télécharger\n",
    "    model_name_gpt2 = \"gpt2\"  \n",
    "    \n",
    "    # Téléchargement du tokenizer associé au modèle GPT-2\n",
    "    tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name_gpt2)\n",
    "    \n",
    "    # Téléchargement du modèle GPT-2 lui-même\n",
    "    model_gpt2 = AutoModelForCausalLM.from_pretrained(model_name_gpt2)\n",
    "    \n",
    "    # Affichage d'un message de confirmation pour GPT-2\n",
    "    print(\"GPT-2 téléchargé avec succès !\")\n",
    "\n",
    "    # Exemple pour GPT-Neo\n",
    "    \n",
    "    # Nom du modèle GPT-Neo à télécharger (ici, la version 125M de paramètres)\n",
    "    model_name_gptneo = \"EleutherAI/gpt-neo-125M\"  # vous pouvez choisir une autre taille\n",
    "    \n",
    "    # Téléchargement du tokenizer associé au modèle GPT-Neo\n",
    "    tokenizer_gptneo = AutoTokenizer.from_pretrained(model_name_gptneo)\n",
    "    \n",
    "    # Téléchargement du modèle GPT-Neo lui-même\n",
    "    model_gptneo = AutoModelForCausalLM.from_pretrained(model_name_gptneo)\n",
    "    \n",
    "    # Affichage d'un message de confirmation pour GPT-Neo\n",
    "    print(\"GPT-Neo téléchargé avec succès !\")\n",
    "\n",
    "# Vérification si le script est exécuté directement (et non importé comme module)\n",
    "if __name__ == \"__main__\":\n",
    "    # Appel de la fonction `telecharger_modeles` pour lancer le téléchargement\n",
    "    telecharger_modeles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58646c0-267d-462f-b773-c9cfeadda321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliothèques importées avec succès !\n",
      "GPT-2 chargé sans problème !\n"
     ]
    }
   ],
   "source": [
    "# Importation de la bibliothèque PyTorch, utilisée pour le calcul tensoriel et les réseaux de neurones\n",
    "import torch\n",
    "\n",
    "# Importation de la bibliothèque Streamlit, utilisée pour créer des applications web interactives\n",
    "import streamlit\n",
    "\n",
    "# Importation des classes nécessaires depuis la bibliothèque `transformers` pour charger les modèles de langage et leurs tokenizers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Affichage d'un message confirmant que les bibliothèques ont été importées avec succès\n",
    "print(\"Bibliothèques importées avec succès !\")\n",
    "\n",
    "# (Optionnel) Test sur GPT-2\n",
    "\n",
    "# Chargement du tokenizer associé au modèle GPT-2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Chargement du modèle GPT-2 lui-même\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Affichage d'un message confirmant que GPT-2 a été chargé sans problème\n",
    "print(\"GPT-2 chargé sans problème !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21b6b8-b4e1-40e6-bc0f-6b1f7b75a5eb",
   "metadata": {},
   "source": [
    "# 2. Génération de texte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38ef34d-43ae-4d7b-8231-54851c0570c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (0.26.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m415.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10a9af9c-00e2-47c6-bcec-aadd8f6c8eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m527.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.12/site-packages (from sacrebleu) (2023.10.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/anaconda3/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from sacrebleu) (1.26.4)\n",
      "Requirement already satisfied: colorama in /opt/anaconda3/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (from sacrebleu) (5.2.1)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m466.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.1.1 sacrebleu-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de571936-4202-48fd-87db-4a4ae0467848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe la bibliothèque PyTorch, utilisée pour le calcul tensoriel et les réseaux de neurones\n",
    "import torch\n",
    "\n",
    "# Importe la classe AutoTokenizer pour charger des tokenizers pré-entraînés\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Importe la classe AutoModelForCausalLM pour charger des modèles de langage causal pré-entraînés\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Importe la fonction corpus_bleu de la bibliothèque sacrebleu pour calculer le score BLEU\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "# Importe la bibliothèque evaluate pour évaluer les modèles de traitement du langage naturel\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "63e2a8d8-5fcc-4ccd-a359-bcda263c9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de la fonction `load_model` qui charge un tokenizer et un modèle de langage\n",
    "def load_model(model_name):\n",
    "    \"\"\"\n",
    "    Charge le tokenizer et le modèle correspondants.\n",
    "    \"\"\"\n",
    "    # Charge le tokenizer associé au modèle spécifié par `model_name`\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Charge le modèle de langage causal (comme GPT-2 ou GPT-Neo) spécifié par `model_name`\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Retourne le tokenizer et le modèle chargés\n",
    "    return tokenizer, model\n",
    "\n",
    "# Exemple d'utilisation de la fonction `load_model` pour charger GPT-2 et GPT-Neo\n",
    "\n",
    "# Charge le tokenizer et le modèle pour GPT-2\n",
    "tokenizer_gpt2, model_gpt2 = load_model(\"gpt2\")\n",
    "\n",
    "# Charge le tokenizer et le modèle pour GPT-Neo (version 125M de paramètres)\n",
    "tokenizer_gptneo, model_gptneo = load_model(\"EleutherAI/gpt-neo-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9ac041a7-4566-4758-bfec-5551712cd0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des classes nécessaires depuis la bibliothèque `transformers`\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# Importation de PyTorch pour gérer les tenseurs et les calculs\n",
    "import torch\n",
    "\n",
    "# Définition de la fonction `generate_text` pour générer du texte à partir d'un prompt\n",
    "def generate_text(tokenizer, model, prompt, max_length=100, temperature=1.0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Génère du texte à partir d'un prompt en utilisant un modèle de langage.\n",
    "    \"\"\"\n",
    "    # Tokenisation du prompt : conversion du texte en entrée en un format tensoriel (compréhensible par le modèle)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Génération du texte\n",
    "    with torch.no_grad():  # Désactive le calcul des gradients pour économiser de la mémoire\n",
    "        output_ids = model.generate(\n",
    "            **inputs,  # Passe les entrées tokenisées au modèle\n",
    "            max_length=max_length,  # Longueur maximale du texte généré\n",
    "            do_sample=True,         # Active l'échantillonnage pour plus de créativité\n",
    "            top_p=top_p,            # Contrôle la diversité des mots générés (échantillonnage par noyau)\n",
    "            temperature=temperature,  # Contrôle la créativité (plus élevé = plus aléatoire)\n",
    "            num_return_sequences=1   # Nombre de séquences à générer\n",
    "        )\n",
    "    \n",
    "    # Décodage des IDs générés en texte lisible\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return generated_text  # Retourne le texte généré\n",
    "\n",
    "# Définition de la fonction `generer_poemes` pour générer des poèmes avec GPT-2 et GPT-Neo\n",
    "def generer_poemes(prompt, max_length=100, temperature=1.0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Génère un poème avec GPT-2 et GPT-Neo en utilisant un prompt optimisé.\n",
    "    \"\"\"\n",
    "    # Chargement des modèles et tokenizers pour GPT-2 et GPT-Neo\n",
    "    tokenizer_gpt2, model_gpt2 = load_model(\"gpt2\")\n",
    "    tokenizer_gptneo, model_gptneo = load_model(\"EleutherAI/gpt-neo-125M\")\n",
    "    \n",
    "    # Génération des poèmes avec GPT-2 et GPT-Neo\n",
    "    poeme_gpt2 = generate_text(tokenizer_gpt2, model_gpt2, prompt, max_length, temperature, top_p)\n",
    "    poeme_gptneo = generate_text(tokenizer_gptneo, model_gptneo, prompt, max_length, temperature, top_p)\n",
    "    \n",
    "    # Retourne les deux poèmes générés\n",
    "    return poeme_gpt2, poeme_gptneo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ed3dde96-79bb-4947-8663-b10bd74ec3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Poème généré par GPT-2 ===\n",
      "Write a romantic free verse poem on the theme of love and freedom. The poem should evoke deep emotions and use poetic language. The poem should be simple and straightforward and say good and bad to your partner, and your partner can answer any questions. If you have never heard of any poem that calls for freedom then this will be a perfect poem.\n",
      "\n",
      "Read a couple-written romance poem to be heard. I am a married woman living in the Midwest of USA.\n",
      "\n",
      "Love in action! If you've never heard of the love poetry for the two songs on the list, you could try\n",
      "\n",
      "=== Poème généré par GPT-Neo ===\n",
      "Write a romantic free verse poem on the theme of love and freedom. The poem should evoke deep emotions and use poetic language. The poem should contain elements of beauty and romance that evoke feelings of inner peace and connection. This poem should be a response to love in the heart of one’s partner, family, or other community.\n",
      "\n",
      "What will your Valentine look like and why is the romance poem you read?\n",
      "\n",
      "This book will help you to capture the love of your life. It will help you to understand what love looks like when you get married. With this book, you will\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt optimisé en anglais pour la génération de poèmes\n",
    "    prompt_user = (\n",
    "        \"Write a romantic free verse poem on the theme of love and freedom. \"\n",
    "        \"The poem should evoke deep emotions and use poetic language.\"\n",
    "    )\n",
    "    \n",
    "    # Paramètres de génération\n",
    "    max_length = 120  # Longueur maximale du poème\n",
    "    temperature = 1.1  # Créativité accrue (plus élevé = plus aléatoire)\n",
    "    top_p = 0.9       # Diversité des mots (échantillonnage par noyau)\n",
    "    \n",
    "    # Génération des poèmes avec GPT-2 et GPT-Neo\n",
    "    poeme_gpt2, poeme_gptneo = generer_poemes(prompt_user, max_length, temperature, top_p)\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(\"=== Poème généré par GPT-2 ===\")\n",
    "    print(poeme_gpt2)\n",
    "    print(\"\\n=== Poème généré par GPT-Neo ===\")\n",
    "    print(poeme_gptneo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "be84b088-41a6-4d31-a51b-fd430072a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(text_ref, text_hyp):\n",
    "    \"\"\"\n",
    "    Calcule un score BLEU (simple) entre un texte de référence et un texte hypothèse.\n",
    "    Le score BLEU est une métrique utilisée pour évaluer la similarité entre deux textes,\n",
    "    souvent utilisée pour évaluer la qualité des traductions ou des générations de texte.\n",
    "    \"\"\"\n",
    "    # Assurez-vous que les entrées sont des chaînes de caractères\n",
    "    reference = text_ref.strip()\n",
    "    hypothesis = text_hyp.strip()\n",
    "\n",
    "    # sacrebleu attend une liste de références\n",
    "    # et l'hypothèse comme une seule chaîne de caractères\n",
    "    bleu = sentence_bleu(hypothesis, [reference])\n",
    "\n",
    "    # Retourne le score BLEU sous forme de float\n",
    "    return bleu.score / 100.0  # sacrebleu retourne un score en pourcentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9e018bc3-15b0-4116-bd25-4f1f119bb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la métrique ROUGE à l'aide de la bibliothèque `evaluate`\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_rouge_score(text_ref, text_hyp):\n",
    "    \"\"\"\n",
    "    Calcule les scores ROUGE (rouge1, rouge2, rougel, etc.) \n",
    "    entre un texte de référence et un texte hypothèse.\n",
    "    ROUGE est une métrique couramment utilisée pour évaluer la qualité des résumés automatiques\n",
    "    ou des générations de texte en comparant la similarité avec un texte de référence.\n",
    "    \"\"\"\n",
    "    # Calcul des scores ROUGE en utilisant la métrique chargée\n",
    "    # `predictions` : liste contenant le texte hypothèse (généré par le modèle)\n",
    "    # `references` : liste contenant le texte de référence (texte de comparaison)\n",
    "    results = rouge_metric.compute(\n",
    "        predictions=[text_hyp],  # Texte hypothèse (généré)\n",
    "        references=[text_ref]    # Texte de référence\n",
    "    )\n",
    "    \n",
    "    # Retourne un dictionnaire contenant les scores ROUGE\n",
    "    # Les clés du dictionnaire sont : 'rouge1', 'rouge2', 'rougeL', etc.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "adf5b7ef-d66b-4c88-a92c-1f5c8863256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparer_textes(text_gpt2, text_gptneo):\n",
    "    \"\"\"\n",
    "    Compare text_gpt2 et text_gptneo en calculant les scores BLEU et ROUGE.\n",
    "    On choisit, par exemple, text_gpt2 comme 'référence' et text_gptneo comme 'hypothèse'.\n",
    "    \"\"\"\n",
    "    # Appelle la fonction pour calculer le score BLEU entre text_gpt2 (référence) et text_gptneo (hypothèse)\n",
    "    bleu = compute_bleu_score(text_gpt2, text_gptneo)\n",
    "    \n",
    "    # Appelle la fonction pour calculer le score ROUGE entre text_gpt2 (référence) et text_gptneo (hypothèse)\n",
    "    rouge = compute_rouge_score(text_gpt2, text_gptneo)\n",
    "    \n",
    "    # Retourne un dictionnaire contenant les scores BLEU et ROUGE\n",
    "    return {\n",
    "        \"BLEU\": bleu,\n",
    "        \"ROUGE\": rouge\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c0f83df7-8e22-46b9-957d-a15550e8f6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats de la comparaison :\n",
      "BLEU = 0.30851415778293173\n",
      "ROUGE = {'rouge1': 0.5333333333333333, 'rouge2': 0.2980769230769231, 'rougeL': 0.38095238095238093, 'rougeLsum': 0.44761904761904764}\n"
     ]
    }
   ],
   "source": [
    "# Vérifie si le script est exécuté directement (et non importé comme module)\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Comparaison des deux poèmes en calculant les scores BLEU et ROUGE\n",
    "    scores = comparer_textes(poeme_gpt2, poeme_gptneo)\n",
    "    \n",
    "    # Affichage des résultats de la comparaison\n",
    "    print(\"Résultats de la comparaison :\")\n",
    "    for k, v in scores.items():\n",
    "        print(f\"{k} = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14db52-8a62-4214-a9f2-73b19c0a775c",
   "metadata": {},
   "source": [
    "## Interpréter les scores obtenus\n",
    "\n",
    "## Score BLEU : 0.3085\n",
    "\n",
    "- **Le score BLEU de 0.3085 indique une similarité modérée entre les textes générés par GPT-2 et GPT-Neo en termes de précision des n-grammes. Ce score suggère que les textes ne sont pas très similaires en termes de structure et de cohérence des phrases.**\n",
    "\n",
    "## Score ROUGE :\n",
    "\n",
    "- **Les scores ROUGE (ROUGE-1 : 0.5333, ROUGE-2 : 0.2981, ROUGE-L : 0.3810, ROUGE-Lsum : 0.4476) montrent une similarité modérée à élevée en termes de mots individuels et de séquences de mots entre les textes générés par les deux modèles. Cela indique que plus de la moitié des mots individuels sont communs, mais la similarité diminue pour les paires de mots et les séquences plus longues.**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6d661-8941-4334-9fa9-9661df4ec4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.1.194:8501\u001b[0m\n",
      "\u001b[0m\n",
      "/opt/anaconda3/lib/python3.12/site-packages/streamlit/util.py:227: RuntimeWarning: coroutine 'expire_cache' was never awaited\n",
      "  pass\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "2025-01-16 23:19:53.974 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
      "2025-01-16 23:19:54.102 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
      "2025-01-16 23:19:54.229 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-01-16 23:21:47.727 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9a10f-ddfd-424e-8104-1f03fe8690db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
